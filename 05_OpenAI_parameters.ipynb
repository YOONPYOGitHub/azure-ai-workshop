{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI 매개변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개요\n",
    "OpenAI 모델에 요청을 보낼 때 여러 매개변수를 사용하여 모델의 동작과 출력을 제어할 수 있습니다. \\\n",
    "이러한 매개변수를 이해하면 텍스트 생성, 질문 응답 또는 기타 사용 사례에 맞게 응답을 세부 조정할 수 있습니다.\n",
    "\n",
    "더 자세한 예제는 공식 문서를 참조하세요: [Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version=\"2024-12-01-preview\"\n",
    ")\n",
    "\n",
    "CHAT_COMPLETIONS_MODEL = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')\n",
    "SEED = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 매개변수: max_tokens\n",
    "**설명**: 생성할 응답의 최대 토큰 수를 설정합니다. \\\n",
    "**기본값**: 16 \\\n",
    "**예제**: max_tokens=50\n",
    "\n",
    "프롬프트의 토큰 수와 max_tokens의 합은 모델의 컨텍스트 길이를 초과할 수 없습니다. \\\n",
    "gpt-4o-mini의 경우 16,384 tokens이며 모델에 따라 다릅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai_with_max_tokens(max_tokens):\n",
    "    response = client.chat.completions.create(\n",
    "          model=CHAT_COMPLETIONS_MODEL,\n",
    "          messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "                    {\"role\":\"user\",\"content\": \"최고의 반려동물은 \"}],\n",
    "                    max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "tokens = [32, 64, 120, 200]\n",
    "for token in tokens:\n",
    "    print(f\"Max Tokens: {token}\\n\")\n",
    "    print(call_openai_with_max_tokens(token))\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 매개변수: temperature\n",
    "\n",
    "**설명**: 출력의 무작위성을 제어합니다. 낮은 값은 출력을 더 결정론적으로 만들고, 높은 값은 무작위성을 증가시킵니다. \\\n",
    "모델이 다음 토큰을 선택할 때, 여러 후보 중에서\n",
    "- 높은 확률 토큰을 쓸지\n",
    "- 조금 낮은 확률 토큰까지 선택할 여지를 줄지\n",
    "\n",
    "**값 범위**: 0에서 2 \\\n",
    "**기본값**: 1 \\\n",
    "**예제**: temperature=0.7\n",
    "\n",
    "높은 값은 모델이 더 창의적인 출력을 생성하도록 합니다. \\\n",
    "창의적인 응용 프로그램에는 0.9를, 명확한 답변이 필요한 경우에는 0(최대 확률 샘플링)을 시도하세요.\n",
    "\n",
    "---\n",
    "**참고**: 일반적으로 이 매개변수 또는 top_p를 조정하되 둘 다 동시에 조정하지 않는 것을 권장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai(num_times, prompt, temperature=0.8, use_seed=False):\n",
    "    for i in range(num_times):\n",
    "        if use_seed:\n",
    "            response = client.chat.completions.create(\n",
    "                model=CHAT_COMPLETIONS_MODEL,\n",
    "                messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "                            {\"role\":\"user\",\"content\": prompt}],\n",
    "                    max_tokens=60,\n",
    "                    seed=SEED,\n",
    "                    temperature = temperature\n",
    "            )\n",
    "        else:\n",
    "            response = client.chat.completions.create(\n",
    "                model=CHAT_COMPLETIONS_MODEL,\n",
    "                messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "                            {\"role\":\"user\",\"content\": prompt}],\n",
    "                    max_tokens=60,\n",
    "                    temperature = temperature\n",
    "            )\n",
    "        print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without seed and temperature, the response is different each time\n",
    "call_openai(10, '최고의 반려동물은 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now using a seed and 0 temperature, the response is the much more consisitent\n",
    "call_openai(10, '최고의 반려동물은 ', temperature = 0, use_seed=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 매개변수: n\n",
    "**설명**: 각 프롬프트에 대해 생성할 응답의 수를 지정합니다. \\\n",
    "**기본값**: 1 \\\n",
    "**예제**: n = 3 \n",
    "\n",
    "---\n",
    "**참고**: 이 매개변수는 여러 응답을 생성하므로 토큰 할당량을 빠르게 소모할 수 있습니다. 신중하게 사용하고 max_tokens 및 stop 설정이 적절한지 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "            model=CHAT_COMPLETIONS_MODEL,\n",
    "            messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "                        {\"role\":\"user\",\"content\": \"최고의 반려동물은\"}],\n",
    "                max_tokens=60,\n",
    "                n=2\n",
    "        )\n",
    "\n",
    "for index, c in enumerate(response.choices):\n",
    "    print(index, c.message.content)\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 매개변수: presence_penalty\n",
    "**설명**: 텍스트에 이미 나타난 토큰을 기반으로 새 토큰에 페널티를 부여하여 모델이 새로운 토큰을 사용하도록 유도합니다. \\\n",
    "**값 범위**: -2.0에서 2.0 \\\n",
    "**기본값**: 0 \\\n",
    "**예제**: presence_penalty=0.5\n",
    "\n",
    "**특징**  \n",
    "- 단어가 *한 번이라도 등장*했으면 그 이후 사용 확률을 낮춤  \n",
    "- \"존재 여부\" 기반이므로 반복 횟수와는 무관  \n",
    "- 표현을 살짝 다양하게 하고 싶을 때 적합  \n",
    "- 자연스러움을 크게 해치지 않으면서 중복을 줄이는 용도\n",
    "\n",
    "**주의 사항**  \n",
    "- 코드 생성·SQL·JSON·Tool Call처럼 **반복이 필수인 출력에서는 사용 금지**  \n",
    "- 주제어가 반복될 수밖에 없는 설명형 프롬프트에서는 효과가 제한적  \n",
    "- 값이 너무 높으면 문맥과 자연스러움이 손상될 수 있음\n",
    "\n",
    "---\n",
    "\n",
    "### presence_penalty vs frequency_penalty 비교\n",
    "\n",
    "| 특징 | presence_penalty | frequency_penalty |\n",
    "|------|------------------|-------------------|\n",
    "| **기준** | 존재 여부 (0/1) | 반복 횟수 (누적) |\n",
    "| **강도** | 약함 | 강함 |\n",
    "| **효과** | 미묘한 변화 | 확실한 억제 |\n",
    "| **용도** | 자연스러운 다양성 | 반복 패턴 제거 |\n",
    "| **예시** | 일반 글쓰기 | 창의적 콘텐츠 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai_with_presence_penalty(presence_penalty):\n",
    "    response = client.chat.completions.create(\n",
    "          model=CHAT_COMPLETIONS_MODEL,\n",
    "          messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"'고양이'에 대한 10문장을 작성해줘. 단, 가능한 자연스럽게 작성해줘.\"}\n",
    "        ],\n",
    "                    max_tokens=400,\n",
    "                    presence_penalty=presence_penalty, \n",
    "                    \n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Generate with different presence_penalty values\n",
    "penalties = [0, 0.5, 1.0, 1.5, 2.0]\n",
    "for penalty in penalties:\n",
    "    print(f\"Presence Penalty: {penalty}\\n\")\n",
    "    print(call_openai_with_presence_penalty(penalty))\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 매개변수: frequency_penalty\n",
    "**설명**: 텍스트에 이미 나타난 빈도를 기반으로 새 토큰에 페널티를 부여하여 동일한 줄을 반복할 가능성을 줄입니다. \\\n",
    "**값 범위**: -2.0에서 2.0 \\\n",
    "**기본값**: 0 \\\n",
    "**예제**: frequency_penalty=0.5\n",
    "\n",
    "**특징**  \n",
    "- 동일 단어가 반복될수록 페널티 강도가 증가  \n",
    "- presence_penalty보다 훨씬 강한 억제 효과  \n",
    "- 반복되는 문장·패턴을 확실하게 줄이고 싶을 때 적합  \n",
    "- 창의적 글쓰기, 마케팅 문구, 스토리텔링에 유용\n",
    "\n",
    "**주의 사항**  \n",
    "- 변수명·키 이름·구문 반복이 중요한 **코드/SQL/JSON 생성에는 절대 사용하면 안 됨**  \n",
    "- 높은 값일수록 다양성은 증가하지만 문맥 일관성은 낮아질 수 있음  \n",
    "- 정보 전달이나 기술적 설명 같은 정확성이 필요한 응답에는 추천하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai_with_frequency_penalty(frequency_penalty):\n",
    "    response = client.chat.completions.create(\n",
    "        model=CHAT_COMPLETIONS_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"'고양이'에 대한 10문장을 작성해줘. 단, 가능한 자연스럽게 작성해줘.\"}\n",
    "        ],\n",
    "        max_tokens=400,\n",
    "        frequency_penalty=frequency_penalty\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Generate with different frequency_penalty values\n",
    "penalties = [0, 0.5, 1.0, 1.5, 2.0]\n",
    "for penalty in penalties:\n",
    "    print(f\"Frequency Penalty: {penalty}\\n\")\n",
    "    print(call_openai_with_frequency_penalty(penalty))\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 탐색할 사용 사례\n",
    "1. **응답 비교**  \n",
    "   여러 응답을 생성하여 사용 사례에 가장 적합한 결과를 선택하세요.\n",
    "\n",
    "2. **다양성 증가**  \n",
    "   창의적 응용 프로그램에서 다양한 표현을 얻기 위해 여러 응답을 생성하세요.\n",
    "\n",
    "3. **강건성 향상**  \n",
    "   여러 응답을 생성하여 일관성과 정확성을 비교·검증하세요.\n",
    "\n",
    "---\n",
    "\n",
    "### 모범 사례\n",
    "1. **프롬프트 길이 최적화**  \n",
    "   프롬프트는 간결하지만 충분한 정보를 담도록 작성하세요.\n",
    "\n",
    "2. **Temperature 및 Top_p 조정**  \n",
    "   결정론적 vs. 창의적 응답의 균형을 위해 온도를 적절히 조정하세요.\n",
    "\n",
    "3. **토큰 사용량 모니터링**  \n",
    "   `max_tokens`를 적절히 설정하여 비용과 응답 길이를 관리하세요.\n",
    "\n",
    "4. **중지 시퀀스 사용**  \n",
    "   모델이 텍스트 생성을 중단해야 하는 지점을 정의해 출력을 안정적으로 제어하세요.\n",
    "\n",
    "5. **여러 응답 생성**  \n",
    "   `n` 매개변수를 사용하여 여러 응답을 생성하고 필요한 응답을 선택하세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
